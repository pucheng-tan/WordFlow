{"created": "2020-10-29 21:26:55.929367", "duration": 2.263495683670044, "exitcode": 0, "root": "group2", "summary": {"passed": 5, "xpassed": 6, "xfailed": 1, "total": 12, "collected": 12}, "tests": [{"nodeid": "PoC/services/test_api_service.py::test_get_collection_valid[Schools-where0]", "outcome": "passed", "metadata": {"id": "IT API.1", "description": "valid query will retrieve valid results"}}, {"nodeid": "PoC/services/test_api_service.py::test_get_collection_valid[Schools-where1]", "outcome": "passed", "metadata": {"id": "IT API.1", "description": "valid query will retrieve valid results"}}, {"nodeid": "PoC/services/test_api_service.py::test_get_collection_invalid[3-invalid where clause]", "outcome": "passed", "metadata": {"id": "IT API.2", "description": "invalid queries of wrong data types will raise the expected Exceptions"}}, {"nodeid": "PoC/services/test_api_service.py::test_get_collection_invalid[valid_collection_name-4]", "outcome": "passed", "metadata": {"id": "IT API.2", "description": "invalid queries of wrong data types will raise the expected Exceptions"}}, {"nodeid": "PoC/services/test_api_service.py::test_get_collection_no_result[Schools-where0]", "outcome": "passed", "metadata": {"id": "IT API.3", "description": "valid query not pointing to an existing document will return empty result"}}, {"nodeid": "PoC/services/test_api_service.py::test_get_where_clause", "lineno": 66, "outcome": "xpassed", "keywords": ["group2", "xfail", "test_get_where_clause", "pytestmark", "PoC/services/test_api_service.py"], "setup": {"duration": 0.0007429000000001018, "outcome": "passed"}, "metadata": {"id": "UT API.1", "description": "pass in data to the _get_where_clause function, see if result is as expected without API call"}, "call": {"duration": 0.00047529999999973427, "outcome": "passed"}, "teardown": {"duration": 0.0004431000000000296, "outcome": "passed"}}, {"nodeid": "PoC/services/test_api_service.py::test_get_subcollection", "lineno": 72, "outcome": "xfailed", "keywords": ["test_get_subcollection", "group2", "xfail", "pytestmark", "PoC/services/test_api_service.py"], "setup": {"duration": 0.0004039000000002346, "outcome": "passed"}, "call": {"duration": 0.00034440000000035553, "outcome": "skipped", "crash": {"path": "D:\\CST Classes\\repos\\usask\\group2\\PoC\\services\\test_api_service.py", "lineno": 85, "message": "Failed: this test hasn't been written yet, so it's marked with xfail. It will be counted towards xpass results in summary, and not actually ran"}, "traceback": [{"path": "PoC\\services\\test_api_service.py", "lineno": 85, "message": "Failed"}], "longrepr": "@pytest.mark.xfail\n    def test_get_subcollection():\n        \"\"\"\n        NOTE: Use xfail mark when the test hasn't been completely written yet.\n            - it will be listed either as \"xfailed\" or \"xpassed\" in test summary\n            - so, testing is not done if you still have xfail or xpass in test summary\n        XFAIL: expected to fail\n            - these tests are still run, but without any traceback reported when it fails\n            - this one has an assert that makes it fail, so it is listed as \"xfailed\" in summary\n            - other tests have no content, so do not actually fail. They are listed as \"xpassed\" in summary\n        \"\"\"\n        # use pytest.fail to fail a test with a given message\n>       pytest.fail(\"this test hasn't been written yet, so it's marked with xfail. It will be counted towards xpass results in summary, and not actually ran\")\nE       Failed: this test hasn't been written yet, so it's marked with xfail. It will be counted towards xpass results in summary, and not actually ran\n\nPoC\\services\\test_api_service.py:85: Failed"}, "teardown": {"duration": 0.00033939999999965664, "outcome": "passed"}}, {"nodeid": "PoC/services/test_api_service.py::test_get_reference_by_field", "lineno": 87, "outcome": "xpassed", "keywords": ["group2", "xfail", "pytestmark", "PoC/services/test_api_service.py", "test_get_reference_by_field"], "setup": {"duration": 0.0003009999999998847, "outcome": "passed"}, "call": {"duration": 0.00024349999999984107, "outcome": "passed"}, "teardown": {"duration": 0.0002572000000000685, "outcome": "passed"}}, {"nodeid": "PoC/services/test_api_service.py::test_limit", "lineno": 91, "outcome": "xpassed", "keywords": ["group2", "xfail", "pytestmark", "PoC/services/test_api_service.py", "test_limit"], "setup": {"duration": 0.00031240000000032353, "outcome": "passed"}, "call": {"duration": 0.0002659000000000411, "outcome": "passed"}, "teardown": {"duration": 0.00035910000000027864, "outcome": "passed"}}, {"nodeid": "PoC/services/test_api_service.py::test_order_by", "lineno": 95, "outcome": "xpassed", "keywords": ["test_order_by", "group2", "xfail", "pytestmark", "PoC/services/test_api_service.py"], "setup": {"duration": 0.00031269999999983256, "outcome": "passed"}, "call": {"duration": 0.00023930000000005336, "outcome": "passed"}, "teardown": {"duration": 0.0005866000000001037, "outcome": "passed"}}, {"nodeid": "PoC/services/test_api_service.py::test_combination", "lineno": 99, "outcome": "xpassed", "keywords": ["test_combination", "group2", "xfail", "pytestmark", "PoC/services/test_api_service.py"], "setup": {"duration": 0.0007749000000001338, "outcome": "passed"}, "call": {"duration": 0.0004013999999998852, "outcome": "passed"}, "teardown": {"duration": 0.0004081999999998587, "outcome": "passed"}}, {"nodeid": "PoC/services/test_api_service.py::test_post", "lineno": 103, "outcome": "xpassed", "keywords": ["group2", "xfail", "test_post", "pytestmark", "PoC/services/test_api_service.py"], "setup": {"duration": 0.0003367000000000786, "outcome": "passed"}, "call": {"duration": 0.00025260000000004723, "outcome": "passed"}, "teardown": {"duration": 0.0005283999999998734, "outcome": "passed"}}], "warnings": [{"message": "this test will break when we add another school", "category": "UserWarning", "when": "runtest", "filename": "D:\\CST Classes\\repos\\usask\\group2\\PoC\\services\\test_api_service.py", "lineno": 39}, {"message": "this test will break when we add another school", "category": "UserWarning", "when": "runtest", "filename": "D:\\CST Classes\\repos\\usask\\group2\\PoC\\services\\test_api_service.py", "lineno": 39}, {"message": "we'll want a mock here", "category": "UserWarning", "when": "runtest", "filename": "D:\\CST Classes\\repos\\usask\\group2\\PoC\\services\\test_api_service.py", "lineno": 65}], "success": false}